Coursera Data Science Specialization - Capstone Presentation
========================================================
author: Kevin E. D'Elia
date: 03/19/2017
autosize: true

Motivation
========================================================

Ever wonder how companies like **Swiftkey** can suggest words to complete that ever-important text on your mobile phone?  Well, the purpose of this Capstone project is to provide some insight into how Natural Language Processing can be used to accomplish that feat.

Students in this course were asked to analyze large bodies of text (**corpora**), perform data mining and data cleansing on the corpora, and develop an algorithm for predicting words given arbitrary text input.  The final product is a Shiny web application which will attempt to mimic the behavior of keyboards on modern mobile devices.

Datasets
========================================================
The datasets used in the initial phase of the project are from a corpus called __HC Corpora__ and are available [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip); information, what little there is, about the corpus can be found at this [site](https://web.archive.org/web/20160522150305/http://www.corpora.heliohost.org/aboutcorpus.html).  The corpora consist of a set of 3 text files, one set each for _English_, _German_, _Russian_, and _Finnish_.  Only the English set was used.  Within the set are the following files:

* en_US.blogs.txt
* en_US.news.txt
* en_US.twitter.txt

For the final application, a set of pre-computed n-gram files were used.  These files were obtained from the [N-grams data](http://www.ngrams.info/intro.asp) website, a reference provided by the Wiki page on n-grams.  The free datasets contain approximately 1 million of the most frequent 2, 3, 4, and 5-grams.

Algorithm
========================================================

Once the n-gram data was loaded, the dataframes were mutated to create a column called **ngram** which consisted of the word~1~ columns concatenated with an underscore; the word~1~ columns were then removed.  The data input via the UI is then "sanitized": underscores are replaced by blanks, punctuation and digits are removed, and the entire string is lowercased.  **Note:** no profane words are scrubbed and stopwords are kept.  The string is then tokenized, blank string possibly resulting from sanitization are removed, the length of the string is calculated, and then the string is formatted with underscores to match the reference data.  Based on the length of the input string, several code blocks are executed, each doing similar work on slightly modified data.  First, the appropriate n-gram table is searched (e.g., the quintgram table is searched for an input quadgram).  If the search returns results (i.e., indices of data which matched the input), those indices are orderd according to frequency.  At most three are selected (the top three) and the data associated with those indices is extracted, the trailing word removed, and that is what is returned to the UI.  If there is no match, the input string is "pruned" (i.e., the first word is removed) and the resulting string is put through the process again.  If no data matches the input string, a message to that effect is returned. 

Shiny Application
========================================================

The Shiny application, which is hosted [here](https://thedatascientist.shinyapps.io/DataCapstone/), consists of two tab panels, both of which are described below:

**Help & Examples**:  This tab briefly describes both the background behind the application and its usage.  Also included is a table which gives the test harness used to validate the functioning of the algorithm.  It consisists primarily of the phrases used and the words predicted by the algorithm.  The application will return the top predictions (three at most) to the user.

**Try it!**:  The UI is a simple interface to the prediction algorithm, consisting of a simple input box for entering your phrase and a button to press to run the prediction algorithm.  Predictions (up to 3) appear below the button.  Note that the algorithm handles most phrases quite well but seems to have some difficulty with strings such as _@@@_ or _%%%_; the application won't crash but neither will it return any predicted values or any indicator to that effect.

While it won't put SwiftKey out of business (just yet, anyway), the application is fast and performs reasonably well for relatively normal, everyday input phrases.